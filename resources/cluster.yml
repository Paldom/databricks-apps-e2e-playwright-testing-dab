resources:
  clusters:
    monitoring_cluster:
      # This is the "named cluster" the jobs run on.
      cluster_name: "${var.cluster_name}-${bundle.target}"

      # Single-node cluster is enough for browser-based monitoring.
      num_workers: 0

      spark_version: "${var.spark_version}"
      node_type_id: "${var.node_type_id}"
      driver_node_type_id: "${var.node_type_id}"

      spark_conf:
        spark.databricks.cluster.profile: singleNode
        spark.master: local[*]

      custom_tags:
        ResourceClass: SingleNode

      autotermination_minutes: 30
      enable_elastic_disk: true

      # Keep this simple; adjust for your workspace security requirements.
      data_security_mode: SINGLE_USER
      single_user_name: ${workspace.current_user.userName}
      runtime_engine: STANDARD
